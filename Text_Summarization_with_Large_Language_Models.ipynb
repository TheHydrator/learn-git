{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "üìù Text Summarization with Large Language Models",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "nileshmalode1_samsum_dataset_text_summarization_path = kagglehub.dataset_download('nileshmalode1/samsum-dataset-text-summarization')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "HuK3oYU_3fgG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-family: Calibri, serif; text-align: center;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 15px solid #1d3580;\n",
        "               width: 100%;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 45;\n",
        "               height: 20%\"><br>\n",
        "    <img src = \"https://i.imgur.com/r3fweff.png\" width = 256, height= 256>\n",
        "    <br><br><br><br><br>\n",
        "    <div style=\"font-size: 62px; color: #02011a\"><b>Text Summarization with<br>Large Language Models</b></div><br>\n",
        "        <hr style=\"border: none;\n",
        "               border-top: 15px solid #1d3580;\n",
        "               width: 100%;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 45;\n",
        "               height: 20%\"> <br>\n",
        "    <div style=\"font-weight: bold;\n",
        "                text-transform: uppercase;\n",
        "                margin-top: 20px;\n",
        "                letter-spacing: 2.5px;\n",
        "                color: #02011a;\n",
        "                \">2023 | <a href =\"https://www.kaggle.com/lusfernandotorres/\">¬© Luis Fernando Torres</a></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Brn_LtKvXhna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 30px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 16px; letter-spacing: 1.5px; color: #02011a\"><b>Table of Contents</b></div>\n",
        "</div>\n",
        "\n",
        "- [Introduction](#intro)<br><br>\n",
        "    - [The Transformer Architecture](#transformers)<br><br>\n",
        "- [This Notebook](#this_notebook)<br><br>\n",
        "    - [The Task](#task)<br><br>\n",
        "    - [The Dataset](#data)<br><br>\n",
        "    - [The Model](#model)<br><br>\n",
        "    - [Evaluation Metrics](#eval)<br><br>    \n",
        "- [Exploring the Dataset](#eda)<br><br>\n",
        "    - [Train Dataset](#train)<br><br>\n",
        "    - [Test Dataset](#test)<br><br>\n",
        "    - [Validation Dataset](#val)<br><br>\n",
        "- [Preprocessing Data](#preprocess)<br><br>\n",
        "- [Modeling](#modeling)<br><br>\n",
        "- [Evaluating and Saving Model](#evaluating)<br><br>\n",
        "- [Conclusion and Deployment](#conclusion)<br><br>"
      ],
      "metadata": {
        "id": "c18rJ5vgXhnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'intro'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2.85px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 56px; letter-spacing: 2.25px;color: #02011a;\"><b>Introduction</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "hAd3YlHJXhnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">November 30<sup>th</sup>, 2022, marks a significant chapter in the History of <b>machine learning</b>. It was the day OpenAI released ChatGPT, setting a new benchmark for chatbots powered by <b>Large Language Models</b> and offering the public an unparalleled conversational experience.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Ever since then, large language models ‚Äî also referred to as <b>LLMs</b> ‚Äî, have been in the public eye due to the extensive number of tasks they are able to perform. Examples include:</p>"
      ],
      "metadata": {
        "id": "umPMOBA6Xhnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style = \"margin-left: 25px;\">\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ Text Summarization</b>: These models are able to perform a summarization of large texts, including legal texts, reviews, dialogues, among many others.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ Sentiment Analysis</b>: They can read through reviews of products and services and classify them as positive, negative, or neutral. These can also be used in Finance to see if the general public feels <i>Bullish</i> or <i>Bearish</i> on certain securities.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ Language Translation</b>: They can provide real-time translations from one language to another.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ Text-based Recommender Systems</b>: They can  also recommend new products for a client based on their reviews on previously bought products.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "DfF5Mb1PXhnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">But how do these models actually work? ü§î</p>"
      ],
      "metadata": {
        "id": "qTymY2qsXhnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'transformers'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 32px; letter-spacing: 2.25px;color: #02011a;\"><b>The Transformer Architecture</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "IEILHjghXhnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">To understand the current state of LLMs, we must go back to Google's 2017 <b>Attention is All You Need</b>. In this paper, the <b>Transformer</b> architecture was introduced to the world, and it changed the industry forever.</p>"
      ],
      "metadata": {
        "id": "ItyzAXYDXhnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <img src = \"https://d2mk45aasx86xg.cloudfront.net/Transformer_architecture_a1d5ffc1e9.webp\" width = 512, height = 950>\n",
        "<p style = \"font-size: 16px;\n",
        "            font-family: 'Georgia', serif;\n",
        "            text-align: center;\n",
        "            margin-top: 10px;\">Transformer architecture. <br>Source: <a href = \"https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power\">Turing</a></p>\n",
        "</center>"
      ],
      "metadata": {
        "id": "fl5XxuvwXhnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">While recurrent neural networks could be used to enable computers to comprehend text, these models were extremely limited due to the fact that they only allowed the machine to process one word at a time, which would result in the model not being able to acquire the full context of a text.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The <b>transformer architecture</b>, however, is based on the attention mechanism, which allows the model to process an entire sentence or paragraph at once, rather than each word at a time. This is the main secret behind the possibility of full context comprehension, which gives much more power to all these language processing models.</p>\n",
        "\n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The processing of text input with the transformer architecture is based on <mark style=\"background-color:#0d0259;\n",
        "          color:white;\n",
        "          border-radius:4px;\n",
        "          opacity:1.0\"><b>tokenization</b></mark>, which is the process of transforming texts into smaller components called tokens. These can be words, subwords, characters, or many others.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The tokens are then mapped to numerical IDs, which are unique for each word or subword. Each ID is then transformed into an <b>embedding</b>: a dense, high-dimensional vector that contains numerical values. These values are designed to capture the original meaning of the tokens and serve as input for the transformer model.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">It is important to note that these embeddings are high-dimensional, with each dimension capturing certain aspects of a token‚Äôs meaning. Due to their high-dimensional nature, embeddings are not easily interpreted by humans, but transformer models readily use them to identify and group together tokens with similar meanings in the vector space.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Take the following example: </p>"
      ],
      "metadata": {
        "id": "X_BhwB54Xhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table style=\"font-family: Calibri, serif; font-size: 18px; letter-spacing: .85px;color: #02011a\">\n",
        "  <tr>\n",
        "    <th><b>Original Text</b></th>\n",
        "    <th><b>Tokenized Text</b></th>\n",
        "    <th><b>Numerical IDs</b></th>\n",
        "    <th><b>Embeddings (First 3 Dimensions)</b></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>As she said this, she looked down at her hands, and was surprised to find<br> that she had put on one of the rabbit's little gloves while she was talking.</td>\n",
        "    <td>['As', ' she', ' said', ' this', ',', ' she', ' looked', ' down', ' at', ' her', ' hands', ',', ' and', ' was', ' surprised', <br>' to', ' find',  ' that', ' she', ' had', ' put', ' on', ' one', ' of', ' the', ' rabbit', \"'s\", ' little', ' gloves', <br>' while', ' she', ' was', ' talking', '.']</td>\n",
        "    <td>['7', ' 22', ' 258', ' 430', $\\dots$, '589', ' 22', ' 78', ' 98', ' 5890']</td>   \n",
        "      <td>['As': [1.12, -0.56, 0.07], <br> ['she': [0.88, 0.45, -2.03], <br> $\\dots$ <br>]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "7qTe8IhAXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">By using this vector as input, the transformer model learns how to generate outputs based on the <b>probabilities of subsequent words that may naturally follow an input word</b>. This process gets repeated until the model creates an entire paragraph starting from an initial statement.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">There is a very intriguing post on Andrej Karpathy's blog, <a href = \"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\"><i><b>The Unreasonable Effectiveness of Recurrent Neural Networks</b></i></a>, that explains why neural networks-based models are effective in predicting the next word of a text. One factor contributing to their effectiveness is the inherent <i>rules</i> in human languages, such as grammar, which constrain word usage in sentences.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">When you feed your model with examples of written language ‚Äî news articles, Twitter/X posts, product reviews, messages, dialogues, etc. ‚Äî it implicitly acquires the rules of language through these examples, which helps it to predict sequences of words and generate human-like texts.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">A large language model ‚Äî such as <i>GPT</i>, <i>BERT</i>, <i>RoBERTa</i>, etc. ‚Äî is a transformer model on a much larger scale. These models are built on an enormous amount of texts, so they learn and become experts in patterns and structures of language. The GPT-4, which is the model behind the premium version of ChatGPT, was trained on massive amounts of text data from the internet, such as books, articles, websites, etc.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">It is also relevant to note that different languages exhibit different patterns and structures. While Western European languages like English, French, German, Spanish, Portuguese, and Italian may share many structural similarities, other languages, such as Arabic and Japanese, are very distinct, posing unique challenges to modeling.</p>"
      ],
      "metadata": {
        "id": "bpQRwu-vXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'this_notebook'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2.85px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 56px; letter-spacing: 2.25px;color: #02011a;\"><b>This Notebook</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "sAhI0ufgXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The goal of this notebook is to demonstrate how Large Language Models can be used for several tasks related to language processing. In this case, I am going to leverage the power of <b>transfer learning</b> to build a model capable of summarizing dialogues.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">For those of you who may not be aware, transfer learning is a machine learning technique in which we use a <i>pre-trained model</i>‚Äîthat is already knowledgeable in a wide domain‚Äîand tailor its expertise for a specific task by training it in a specific dataset we might have. This process may also be referred to as <b>fine-tuning</b>.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The <a href = \"https://huggingface.co/docs/transformers/index\"><b>ü§ó Transformers</b></a> library‚Äîwhich is one of the most popular libraries for working with deep learning tasks‚Äîoffers the possibility of working with the following architectures:</p>"
      ],
      "metadata": {
        "id": "mRLvtzaNXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table style=\"font-family: Calibri, serif; font-size: 20px; letter-spacing: .85px;color: #02011a; float: left;\">\n",
        "  <tr>\n",
        "    <th><b>Model Architectures</b></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>BART, BigBird-Pegasus, Blenderbot, BlenderbotSmall, Encoder decoder, <br> FairSeq Machine-Translation, GPTSAN-japanese, LED, LongT5, M2M100, Marian, <br>mBART, MT5, MVP, NLLB, NLLB-MOE, Pegasus, <br>PEGASUS-X, PLBart, ProphetNet, SwitchTransformers, T5, UMT5, <br>XLM-ProphetNet</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "j7iA0mdVXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The <b>ü§ó Transformers</b> library allows us to easily download and fine-tune state-of-the-art pre-trained models, and also allows us to easily work with both <b>TensorFlow</b> and <b>PyTorch</b> for several tasks related to Natural Language Preprocessing, Computer Vision, Audio, etc.</p>"
      ],
      "metadata": {
        "id": "0RK9NlW0Xhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'task'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 32px; letter-spacing: 2.25px;color: #02011a;\"><b>The Task</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "3ihU4rgRXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">As previously mentioned, the task at hand is <b>Text Summarization</b>. From the documentation of the ü§ó Transformers library, summarization can be described as the creation of <i>a shorter version of a document or an article that captures all the important information</i>.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">In this case, we are going to summarize dialogues by using a dataset containing chat texts.</p>"
      ],
      "metadata": {
        "id": "LBbK33EgXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">For this task, we are going to use the <a href = \"https://www.kaggle.com/datasets/nileshmalode1/samsum-dataset-text-summarization/versions/1\"><b>SamSum Dataset</b></a>, which contains three <i>csv</i> files for training, testing, and validation. All these files are structured into a specific <code>id</code>, a <code>dialogue</code>, and a <code>summary</code>. The SamSum dataset consists of chat texts, which is ideal for the summarization of dialogues.</p>\n",
        "     "
      ],
      "metadata": {
        "id": "94uJEJnbXhne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'model'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 32px; letter-spacing: 2.25px;color: #02011a;\"><b>The Model</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "uYETIo5LXhnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">As previously mentioned, we are going to harness the power of a pre-trained model for this task. In this case, I have decided to use the <b>BART</b> architecture, proposed in the  2019 paper <a href = \"https://arxiv.org/abs/1910.13461\">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>. More specifically, I am going to fine-tune a version of BART that has been already trained to perform text summarization of news articles, which is the <a href =\"https://huggingface.co/facebook/bart-large-xsum\"><b>facebook/bart-large-xsum</b></a> version.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Briefly explaining, BART is a denoising autoencoder that employs the strategy of distorting the input text in many ways, such as blanking out some words and flipping them around, and then learning to reconstruct it. BART has outperformed established models like RoBERTa and BERT on multiple NLP benchmarks, and it is especially efficient in summarization tasks, due to its ability to generate text and learn the context of the input text.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">For a deeper comprehension of BART, I highly suggest you read the research paper linked above, where this architecture was first introduced.</p>"
      ],
      "metadata": {
        "id": "5cZXfEFdXhnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'eval'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 32px; letter-spacing: 2.25px;color: #02011a;\"><b>Evaluation Metrics</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "uGYG0DBiXhnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Evaluating performance for language models can be quite tricky, especially when it comes to text summarization. The goal of our model is to produce a short sentence describing the content of a dialogue, while maintaining all the important information within that dialogue.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">One of the quantitative metrics we can employ to evaluate performance is the <b>ROUGE Score</b>. It is considered one of the best metrics for text summarization and it evaluates performance by comparing the quality of a machine-generated summary to a human-generated summary used for reference.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The similarities between both summaries are measured by analyzing the overlapping <i>n-grams</i>, either single words or sequences of words that are present in both summaries. These can be unigrams (ROUGE-1), where only the overlap of sole words is measured; bigrams (ROUGE-2), where we measure the overlap of two-word sequences; trigrams (ROUGE-3), where we measure the overlap of three-word sequences; etc. Besides that, we also have:</p>"
      ],
      "metadata": {
        "id": "OYy7mHquXhnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style = \"margin-left: 25px;\">\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ ROUGE-L</b>: It measures the <i>Longest Common Subsequence (LCS)</i> between the two summaries, which helps to capture content coverage of the machine-generated text. If both summaries have the sequence <i>\"the apple is green\"</i>, we have a match regardless of where they appear in both texts.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ ROUGE-S</b>: It evaluates the overlap of skip-bigrams, which are bigrams that permit gaps between words. This helps to measure the coherence of a machine-generated summary. For example, in the phrase <i>\"this apple is absolutely green\"</i>, we find a match for the terms such as <i>\"apple\"</i> and <i>\"green\"</i>, if that is what we are looking for.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Z8BR3n5tXhnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">These scores might typically range from 0 to 100, where 0 indicates no match and 100 indicates a perfect match between both summaries.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Besides quantitative metrics, it is useful to use <b>human evaluation</b> to analyze the output of language models, since we are able to comprehend text in a way that a machine does not. So we might read the dialogue and then read the summary to check if it is an accurate summarization.</p>"
      ],
      "metadata": {
        "id": "BvotrkWIXhnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi # Checking GPU"
      ],
      "metadata": {
        "id": "JeFtwh5EkxHI",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-10-30T19:30:49.938615Z",
          "iopub.execute_input": "2023-10-30T19:30:49.939111Z",
          "iopub.status.idle": "2023-10-30T19:30:51.040819Z",
          "shell.execute_reply.started": "2023-10-30T19:30:49.939078Z",
          "shell.execute_reply": "2023-10-30T19:30:51.039696Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers # Installing the transformers library (https://huggingface.co/docs/transformers/index)"
      ],
      "metadata": {
        "id": "hTmEpjKaXrfg",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-10-30T19:30:51.043008Z",
          "iopub.execute_input": "2023-10-30T19:30:51.043311Z",
          "iopub.status.idle": "2023-10-30T19:31:04.253871Z",
          "shell.execute_reply.started": "2023-10-30T19:30:51.043285Z",
          "shell.execute_reply": "2023-10-30T19:31:04.252617Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets # Installing the datasets library (https://huggingface.co/docs/datasets/index)"
      ],
      "metadata": {
        "id": "hbEqEC6UXpcn",
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-10-30T19:31:04.255648Z",
          "iopub.execute_input": "2023-10-30T19:31:04.256149Z",
          "iopub.status.idle": "2023-10-30T19:31:16.392152Z",
          "shell.execute_reply.started": "2023-10-30T19:31:04.256084Z",
          "shell.execute_reply": "2023-10-30T19:31:16.391059Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate # Installing the evaluate library (https://huggingface.co/docs/evaluate/main/en/index)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "id": "7Eqnrw3xXhng",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:45:38.754953Z",
          "iopub.execute_input": "2023-10-31T19:45:38.755385Z",
          "iopub.status.idle": "2023-10-31T19:45:54.37761Z",
          "shell.execute_reply.started": "2023-10-31T19:45:38.755349Z",
          "shell.execute_reply": "2023-10-31T19:45:54.376181Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score # Installing rouge-score library (https://pypi.org/project/rouge-score/)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T19:31:29.360815Z",
          "iopub.execute_input": "2023-10-30T19:31:29.361146Z",
          "iopub.status.idle": "2023-10-30T19:31:44.160921Z",
          "shell.execute_reply.started": "2023-10-30T19:31:29.361119Z",
          "shell.execute_reply": "2023-10-30T19:31:44.159726Z"
        },
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "6lq0vfcM3fgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py7zr # Installing library to save zip archives (https://pypi.org/project/py7zr/)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T19:31:44.162515Z",
          "iopub.execute_input": "2023-10-30T19:31:44.162837Z",
          "iopub.status.idle": "2023-10-30T19:31:59.390387Z",
          "shell.execute_reply.started": "2023-10-30T19:31:44.162809Z",
          "shell.execute_reply": "2023-10-30T19:31:59.38918Z"
        },
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "wYS1TOb63fgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_metric\n",
        "import shutil\n",
        "\n",
        "# Data Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import plotly.subplots as sp\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.io as pio\n",
        "from IPython.display import display\n",
        "from plotly.offline import init_notebook_mode\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "# Statistics & Mathematics\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import shapiro, skew, anderson, kstest, gaussian_kde,spearmanr\n",
        "import math\n",
        "\n",
        "# Hiding warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "Uz-FPCeZXhnf",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:43:42.854637Z",
          "iopub.execute_input": "2023-10-31T19:43:42.856616Z",
          "iopub.status.idle": "2023-10-31T19:43:48.700297Z",
          "shell.execute_reply.started": "2023-10-31T19:43:42.856485Z",
          "shell.execute_reply": "2023-10-31T19:43:48.699056Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration      # BERT Tokenizer and architecture\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments         # These will help us to fine-tune our model\n",
        "from transformers import pipeline                                         # Pipeline\n",
        "from transformers import DataCollatorForSeq2Seq                           # DataCollator to batch the data\n",
        "import torch                                                              # PyTorch\n",
        "import evaluate                                                           # Hugging Face's library for model evaluation\n",
        "\n",
        "\n",
        "# Other NLP libraries\n",
        "from textblob import TextBlob                                             # This is going to help us fix spelling mistakes in texts\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer               # This is going to helps identify the most common terms in the corpus\n",
        "import re                                                                 # This library allows us to clean text data\n",
        "import nltk                                                               # Natural Language Toolkit\n",
        "nltk.download('punkt')                                                    # This divides a text into a list of sentences"
      ],
      "metadata": {
        "id": "WddFozUqXhng",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:45:54.38001Z",
          "iopub.execute_input": "2023-10-31T19:45:54.380391Z",
          "iopub.status.idle": "2023-10-31T19:45:55.207777Z",
          "shell.execute_reply.started": "2023-10-31T19:45:54.380358Z",
          "shell.execute_reply": "2023-10-31T19:45:55.206524Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">By observing the imports above, you can clearly note that I have choosen to work with <b>PyTorch</b> for this notebook.</p>"
      ],
      "metadata": {
        "id": "R6TVsk5fXhng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring Pandas to exhibit larger columns\n",
        "'''\n",
        "This is going to allow us to fully read the dialogues and their summary\n",
        "'''\n",
        "pd.set_option('display.max_colwidth', 1000)"
      ],
      "metadata": {
        "id": "ROlKWwoYXhng",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:19.515724Z",
          "iopub.execute_input": "2023-10-30T19:32:19.516036Z",
          "iopub.status.idle": "2023-10-30T19:32:19.522062Z",
          "shell.execute_reply.started": "2023-10-30T19:32:19.516011Z",
          "shell.execute_reply": "2023-10-30T19:32:19.520875Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring notebook\n",
        "seed = 42\n",
        "#paper_color =\n",
        "#bg_color =\n",
        "colormap = 'cividis'\n",
        "template = 'plotly_dark'"
      ],
      "metadata": {
        "id": "hYkd3fs5Xhng",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:43:56.907237Z",
          "iopub.execute_input": "2023-10-31T19:43:56.907963Z",
          "iopub.status.idle": "2023-10-31T19:43:56.913963Z",
          "shell.execute_reply.started": "2023-10-31T19:43:56.907922Z",
          "shell.execute_reply": "2023-10-31T19:43:56.912688Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available. \\nUsing GPU\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"GPU is not available. \\nUsing CPU\")\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "IiG7bNqhXhng",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:19.541474Z",
          "iopub.execute_input": "2023-10-30T19:32:19.541826Z",
          "iopub.status.idle": "2023-10-30T19:32:19.580254Z",
          "shell.execute_reply.started": "2023-10-30T19:32:19.541793Z",
          "shell.execute_reply": "2023-10-30T19:32:19.579283Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_feature_list(features, feature_type):\n",
        "\n",
        "    '''\n",
        "    This function displays the features within each list for each type of data\n",
        "    '''\n",
        "\n",
        "    print(f\"\\n{feature_type} Features: \")\n",
        "    print(', '.join(features) if features else 'None')\n",
        "\n",
        "def describe_df(df):\n",
        "    \"\"\"\n",
        "    This function prints some basic info on the dataset and\n",
        "    sets global variables for feature lists.\n",
        "    \"\"\"\n",
        "\n",
        "    global categorical_features, continuous_features, binary_features\n",
        "    categorical_features = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    binary_features = [col for col in df.columns if df[col].nunique() <= 2 and df[col].dtype != 'object']\n",
        "    continuous_features = [col for col in df.columns if df[col].dtype != 'object' and col not in binary_features]\n",
        "\n",
        "    print(f\"\\n{type(df).__name__} shape: {df.shape}\")\n",
        "    print(f\"\\n{df.shape[0]:,.0f} samples\")\n",
        "    print(f\"\\n{df.shape[1]:,.0f} attributes\")\n",
        "    print(f'\\nMissing Data: \\n{df.isnull().sum()}')\n",
        "    print(f'\\nDuplicates: {df.duplicated().sum()}')\n",
        "    print(f'\\nData Types: \\n{df.dtypes}')\n",
        "\n",
        "    #negative_valued_features = [col for col in df.columns if (df[col] < 0).any()]\n",
        "    #print(f'\\nFeatures with Negative Values: {\", \".join(negative_valued_features) if negative_valued_features else \"None\"}')\n",
        "\n",
        "    display_feature_list(categorical_features, 'Categorical')\n",
        "    display_feature_list(continuous_features, 'Continuous')\n",
        "    display_feature_list(binary_features, 'Binary')\n",
        "\n",
        "    print(f'\\n{type(df).__name__} Head: \\n')\n",
        "    display(df.head(5))\n",
        "    print(f'\\n{type(df).__name__} Tail: \\n')\n",
        "    display(df.tail(5))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "VhD-ueuPXhng",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:19.581464Z",
          "iopub.execute_input": "2023-10-30T19:32:19.581765Z",
          "iopub.status.idle": "2023-10-30T19:32:19.592752Z",
          "shell.execute_reply.started": "2023-10-30T19:32:19.581741Z",
          "shell.execute_reply": "2023-10-30T19:32:19.591944Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_boxplot(df,hist_color, box_color, height, width, legend, name):\n",
        "    '''\n",
        "    This function plots a Histogram and a Box Plot side by side\n",
        "\n",
        "    Parameters:\n",
        "    hist_color = The color of the histogram\n",
        "    box_color = The color of the boxplots\n",
        "    heigh and width = Image size\n",
        "    legend = Either to display legend or not\n",
        "    '''\n",
        "\n",
        "    features = df.select_dtypes(include = [np.number]).columns.tolist()\n",
        "\n",
        "    for feat in features:\n",
        "        try:\n",
        "            fig = make_subplots(\n",
        "                rows=1,\n",
        "                cols=2,\n",
        "                subplot_titles=[\"Box Plot\", \"Histogram\"],\n",
        "                horizontal_spacing=0.2\n",
        "            )\n",
        "\n",
        "            density = gaussian_kde(df[feat])\n",
        "            x_vals = np.linspace(min(df[feat]), max(df[feat]), 200)\n",
        "            density_vals = density(x_vals)\n",
        "\n",
        "            fig.add_trace(go.Scatter(x=x_vals, y = density_vals, mode = 'lines',\n",
        "                                     fill = 'tozeroy', name=\"Density\", line_color=hist_color), row=1, col=2)\n",
        "            fig.add_trace(go.Box(y=df[feat], name=\"Box Plot\", boxmean=True, line_color=box_color), row=1, col=1)\n",
        "\n",
        "            fig.update_layout(title={'text': f'<b>{name} Word Count<br><sup><i>&nbsp;&nbsp;&nbsp;&nbsp;{feat}</i></sup></b>',\n",
        "                                     'x': .025, 'xanchor': 'left'},\n",
        "                             margin=dict(t=100),\n",
        "                             showlegend=legend,\n",
        "                             template = template,\n",
        "                             #plot_bgcolor=bg_color,paper_bgcolor=paper_color,\n",
        "                             height=height, width=width\n",
        "                            )\n",
        "\n",
        "            fig.update_yaxes(title_text=f\"<b>Words</b>\", row=1, col=1, showgrid=False)\n",
        "            fig.update_xaxes(title_text=\"\", row=1, col=1, showgrid=False)\n",
        "\n",
        "            fig.update_yaxes(title_text=\"<b>Frequency</b>\", row=1, col=2,showgrid=False)\n",
        "            fig.update_xaxes(title_text=f\"<b>Words</b>\", row=1, col=2, showgrid=False)\n",
        "\n",
        "            fig.show()\n",
        "            print('\\n')\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "gyLxoXpqXhng",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:19.594002Z",
          "iopub.execute_input": "2023-10-30T19:32:19.594385Z",
          "iopub.status.idle": "2023-10-30T19:32:19.608688Z",
          "shell.execute_reply.started": "2023-10-30T19:32:19.594355Z",
          "shell.execute_reply": "2023-10-30T19:32:19.60778Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_correlation(df, title, subtitle, height, width, font_size):\n",
        "    '''\n",
        "    This function is resposible to plot a correlation map among features in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    height = Define height\n",
        "    width = Define width\n",
        "    font_size = Define the font size for the annotations\n",
        "    '''\n",
        "    corr = np.round(df.corr(numeric_only = True), 2)\n",
        "    mask = np.triu(np.ones_like(corr, dtype = bool))\n",
        "    c_mask = np.where(~mask, corr, 100)\n",
        "\n",
        "    c = []\n",
        "    for i in c_mask.tolist()[1:]:\n",
        "        c.append([x for x in i if x != 100])\n",
        "\n",
        "\n",
        "\n",
        "    fig = ff.create_annotated_heatmap(z=c[::-1],\n",
        "                                      x=corr.index.tolist()[:-1],\n",
        "                                      y=corr.columns.tolist()[1:][::-1],\n",
        "                                      colorscale = colormap)\n",
        "\n",
        "    fig.update_layout(title = {'text': f\"<b>{title} Heatmap<br><sup>&nbsp;&nbsp;&nbsp;&nbsp;<i>{subtitle}</i></sup></b>\",\n",
        "                                'x': .025, 'xanchor': 'left', 'y': .95},\n",
        "                    margin = dict(t=210, l = 110),\n",
        "                    yaxis = dict(autorange = 'reversed', showgrid = False),\n",
        "                    xaxis = dict(showgrid = False),\n",
        "                    template = template,\n",
        "                    #plot_bgcolor=bg_color,paper_bgcolor=paper_color,\n",
        "                    height = height, width = width)\n",
        "\n",
        "\n",
        "    fig.add_trace(go.Heatmap(z = c[::-1],\n",
        "                             colorscale = colormap,\n",
        "                             showscale = True,\n",
        "                             visible = False))\n",
        "    fig.data[1].visible = True\n",
        "\n",
        "    for i in range(len(fig.layout.annotations)):\n",
        "        fig.layout.annotations[i].font.size = font_size\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "hlOOzo_iXhnh",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:44:09.165496Z",
          "iopub.execute_input": "2023-10-31T19:44:09.165967Z",
          "iopub.status.idle": "2023-10-31T19:44:09.180142Z",
          "shell.execute_reply.started": "2023-10-31T19:44:09.165933Z",
          "shell.execute_reply": "2023-10-31T19:44:09.178896Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(df_column, ngram_range=(1,1), max_features=15):\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', ngram_range=ngram_range)\n",
        "    x = vectorizer.fit_transform(df_column.fillna(''))\n",
        "    df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "    return df_tfidfvect"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "sw6RuZJIXhnh",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:44:13.163838Z",
          "iopub.execute_input": "2023-10-31T19:44:13.164242Z",
          "iopub.status.idle": "2023-10-31T19:44:13.170999Z",
          "shell.execute_reply.started": "2023-10-31T19:44:13.164212Z",
          "shell.execute_reply": "2023-10-31T19:44:13.169965Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'eda'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2.85px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 56px; letter-spacing: 2.25px;color: #02011a;\"><b>Exploring the Dataset</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "WV0DsFEfXhnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We can start our analysis of the dataset by loading all the three sets available, <code>train</code>, <code>test</code>, and <code>val</code>.</p>"
      ],
      "metadata": {
        "id": "g1WbV2qzXhnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "train = pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-train.csv')\n",
        "test = pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv')\n",
        "val = pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv')"
      ],
      "metadata": {
        "id": "bbv9GEAvXhno",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:44:17.406066Z",
          "iopub.execute_input": "2023-10-31T19:44:17.406781Z",
          "iopub.status.idle": "2023-10-31T19:44:17.778463Z",
          "shell.execute_reply.started": "2023-10-31T19:44:17.406744Z",
          "shell.execute_reply": "2023-10-31T19:44:17.777405Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I am now going to analyze each dataset separately.</p>"
      ],
      "metadata": {
        "id": "mb481KRRXhno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'train'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 32px; letter-spacing: 2.25px;color: #02011a;\"><b>Train Dataset</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "M0Eocv3RXhno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting info on the training Dataframe\n",
        "describe_df(train)"
      ],
      "metadata": {
        "id": "Hd0LbMaRXhno",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:19.983803Z",
          "iopub.execute_input": "2023-10-30T19:32:19.984514Z",
          "iopub.status.idle": "2023-10-30T19:32:20.056433Z",
          "shell.execute_reply.started": "2023-10-30T19:32:19.984477Z",
          "shell.execute_reply": "2023-10-30T19:32:20.05545Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We have 14,732 pairs of dialogues and summaries. It also seems like one of the dialogues is empty, let's investigate it further.</p>"
      ],
      "metadata": {
        "id": "hJi2B16XXhno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = train['dialogue'].isnull() # Creating mask with null dialogues\n",
        "filtered_train = train[mask] # filtering dataframe\n",
        "filtered_train # Visualizing"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "7X8cKVJ4Xhno",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:44:53.824672Z",
          "iopub.execute_input": "2023-10-31T19:44:53.825098Z",
          "iopub.status.idle": "2023-10-31T19:44:53.853476Z",
          "shell.execute_reply.started": "2023-10-31T19:44:53.825067Z",
          "shell.execute_reply": "2023-10-31T19:44:53.852329Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">It seems that sample <b>6054</b> does not really add anything to the dataset. We have a Null dialogue and the summary does not give us a clue on what this dialogue was supposed to be. We will remove this entry.</p>"
      ],
      "metadata": {
        "id": "3gjQK66HXhno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.dropna() # removing null values"
      ],
      "metadata": {
        "id": "bCEPWHFYXhno",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:44:57.25334Z",
          "iopub.execute_input": "2023-10-31T19:44:57.253754Z",
          "iopub.status.idle": "2023-10-31T19:44:57.269493Z",
          "shell.execute_reply.started": "2023-10-31T19:44:57.253719Z",
          "shell.execute_reply": "2023-10-31T19:44:57.268266Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'Id' from categorical features list\n",
        "categorical_features.remove('id')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "t5hWlzDmXhno",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:20.090515Z",
          "iopub.execute_input": "2023-10-30T19:32:20.090836Z",
          "iopub.status.idle": "2023-10-30T19:32:20.0974Z",
          "shell.execute_reply.started": "2023-10-30T19:32:20.0908Z",
          "shell.execute_reply": "2023-10-30T19:32:20.096554Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We can now analyze the length of both dialogues and summaries by counting the words in them. This might give us a clue about how these texts are structured.</p>"
      ],
      "metadata": {
        "id": "RM2dTK4BXhno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_text_lenght = pd.DataFrame() # Creating an empty dataframe\n",
        "for feat in categorical_features: # Iterating through features --> Dialogue & Summary\n",
        "    df_text_lenght[feat] = train[feat].apply(lambda x: len(str(x).split())) #  Counting words for each feature\n",
        "\n",
        "# Plotting histogram-boxplot\n",
        "histogram_boxplot(df_text_lenght,'#89c2e0', '#d500ff', 600, 1000, True, 'Train Dataset')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "_taXWhmAXhno",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:20.098431Z",
          "iopub.execute_input": "2023-10-30T19:32:20.098707Z",
          "iopub.status.idle": "2023-10-30T19:32:21.744226Z",
          "shell.execute_reply.started": "2023-10-30T19:32:20.098683Z",
          "shell.execute_reply": "2023-10-30T19:32:21.743277Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">On average, dialogues consist of about 94 words. We do have some outliers with very extensive texts, going way over 300 words per dialogue.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Summaries are naturally shorter texts, consisting of about 20 words on average, although we also have some outliers with extensive summaries.</p>"
      ],
      "metadata": {
        "id": "vXbLp4FHXhno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We can also use scikit-learn's <code>TfidfVectorizer</code> to extract more info on the dialogues and summaries available. This function will give us a dataframe with the top $n$ most frequent terms in the corpus, which we select by using the <code>max_features</code> parameter.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">In this dataframe, each column represents the $n$ most frequent terms in the overall corpus, while each row represents one entry in the original dataframe, such as <code>train</code>. For each term in each entry, we will see the TF-IDF score associated with it, which quantifies the relevance of a term in a given dialogue ‚Äî or summary ‚Äî relative to its frequency across all other dialogues ‚Äî or summaries.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We will also use the <code>ngram_range</code> parameter to select the most frequent words (unigrams), the most frequent sequence of two words (bigrams), and the most frequent sequence of three words (trigrams). The <code>stop_words = 'english'</code> parameter will help us filter out common stop-words of the English language, which are words that do not add up much to the overall context, such as <i>\"and\"</i>, <i>\"of\"</i>, etc.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">After measuring the most frequent terms, I will plot a heatmap displaying the correlations between these terms. This may help us understand how frequently they are used together in dialogues. For instance, how frequent is the occurrence of the word <i>\"will\"</i> when the word <i>\"we\"</i> is present?</p>"
      ],
      "metadata": {
        "id": "uwabyqDSXhno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english') # Top 15 terms\n",
        "x = vectorizer.fit_transform(train['dialogue'])\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Unigrams', 'Train - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "PyuJt4cuXhnp",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:46:24.495256Z",
          "iopub.execute_input": "2023-10-31T19:46:24.495648Z",
          "iopub.status.idle": "2023-10-31T19:46:27.617229Z",
          "shell.execute_reply.started": "2023-10-31T19:46:24.495619Z",
          "shell.execute_reply": "2023-10-31T19:46:27.61614Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">You can see that the correlations between these terms are neither strongly positive nor strongly negative. The most positively correlated terms are <i>\"don\"</i> and <i>\"know\"</i>, at 0.12. It is relevant to observe that the <code>TfidfVectorizer</code> function performs some changes to the text, such as removing contractions, which explains why the word <i>don't</i> appears without its apostrophe <i>'t</i>.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">It is also interesting to notice a negative correlation ‚Äî although still not extremely significant ‚Äî between the terms <i>\"yes\"</i> and <i>\"yeah\"</i>. Maybe this happens because it would be redundant to include both in the same dialogue, or perhaps the data captures a tendency of individuals to use <i>\"yeah\"</i> instead of <i>\"yes\"</i> during conversations. These are some hypotheses we can consider when analyzing this type of heatmaps.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's perform the same analysis to the summaries.</p>"
      ],
      "metadata": {
        "id": "ODA5q-1WXhnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english') # Top 15 terms\n",
        "x = vectorizer.fit_transform(train['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Unigrams', 'Train - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "L0QZOrCJXhnp",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:46:34.885304Z",
          "iopub.execute_input": "2023-10-31T19:46:34.885691Z",
          "iopub.status.idle": "2023-10-31T19:46:35.597229Z",
          "shell.execute_reply.started": "2023-10-31T19:46:34.885663Z",
          "shell.execute_reply": "2023-10-31T19:46:35.595961Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The correlations of terms in summaries seem to be more pronounced than those in dialogues, even though these correlations are still not strong. This suggests that summaries may convey relevant information more succinctly than full dialogues, which is exactly the idea behind a summary.</p>\n",
        "\n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We have positively correlated pairs such as <i>\"going\"</i> and <i>\"meet\"</i>, <i>\"come\"</i> and <i>\"party\"</i>, as well as <i>\"buy\"</i> and <i>\"wants\"</i>. It makes perfect sense to see these unigrams appearing together across texts.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Conversely, it's reasonable for negatively correlated pairs <b>not</b> to co-occur frequently in texts, such as <i>\"going\"</i> and <i>\"wants\"</i>, and <i>\"going\"</i> and <i>\"got\"</i>.</p>"
      ],
      "metadata": {
        "id": "DXN5B_03Xhnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's now analyze bigrams across dialogues and summaries.</p>"
      ],
      "metadata": {
        "id": "06FnqacHXhnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (2,2)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(train['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Bigrams', 'Train - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "GT6qhvgmXhnp",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:46:41.913146Z",
          "iopub.execute_input": "2023-10-31T19:46:41.91356Z",
          "iopub.status.idle": "2023-10-31T19:46:46.49901Z",
          "shell.execute_reply.started": "2023-10-31T19:46:41.913528Z",
          "shell.execute_reply": "2023-10-31T19:46:46.497686Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Once more, the correlations are not extremely strong. Still, we can see some pairs that seem reasonable to be together, such as <i>\"good idea\"</i> and <i>\"sounds like\"</i>.</p>"
      ],
      "metadata": {
        "id": "wAjCvT3QXhnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (2,2)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(train['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Bigrams', 'Train - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "ltc7vnf1Xhnp",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:47:01.083722Z",
          "iopub.execute_input": "2023-10-31T19:47:01.084202Z",
          "iopub.status.idle": "2023-10-31T19:47:02.396688Z",
          "shell.execute_reply.started": "2023-10-31T19:47:01.084166Z",
          "shell.execute_reply": "2023-10-31T19:47:02.395777Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We have only one correlation between the pairs <i>\"wants buy\"</i> and <i>\"buy new\"</i>. The other terms do not appear to have any kind of correlation at all.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">It is interesting to see the tendency of the summaries to contain information on minutes, which does not seem to be present in the dialogues. We can even investigate further this relationship by querying some summaries where the bigram <i>15 minutes</i> appears in the summary.</p>"
      ],
      "metadata": {
        "id": "WQSL9MGqXhnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering dataset to see those containing the term '15 minutes' in the summary\n",
        "filtered_train = train[train['summary'].str.contains('15 minutes', case=False, na=False)]\n",
        "filtered_train.head()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "OQRdfUdyXhnq",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:29.21069Z",
          "iopub.execute_input": "2023-10-30T19:32:29.211078Z",
          "iopub.status.idle": "2023-10-30T19:32:29.235917Z",
          "shell.execute_reply.started": "2023-10-30T19:32:29.211045Z",
          "shell.execute_reply": "2023-10-30T19:32:29.234936Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The last row gives us an idea of why we see so many terms related to minutes in summaries, but not in dialogues. In dialogues, people may write \"15min\" together or even other forms of it, such as \"15m\", whereas the summaries give us a patternized description, making it natural to be more prominent than other forms to describe time.</p>"
      ],
      "metadata": {
        "id": "Z8LluRrHXhnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's now visualize the trigrams.</p>"
      ],
      "metadata": {
        "id": "L08msyHrXhnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (3,3)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(train['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Trigrams', 'Train - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "RJzE-zEYXhnq",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:47:07.815525Z",
          "iopub.execute_input": "2023-10-31T19:47:07.816527Z",
          "iopub.status.idle": "2023-10-31T19:47:13.32837Z",
          "shell.execute_reply.started": "2023-10-31T19:47:07.816485Z",
          "shell.execute_reply": "2023-10-31T19:47:13.327148Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (3,3)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(train['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Trigrams', 'Train - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "6bNstxPnXhnq",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:47:13.330613Z",
          "iopub.execute_input": "2023-10-31T19:47:13.331268Z",
          "iopub.status.idle": "2023-10-31T19:47:14.675245Z",
          "shell.execute_reply.started": "2023-10-31T19:47:13.331223Z",
          "shell.execute_reply": "2023-10-31T19:47:14.674028Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Once more, we can see that the terms are not strongly correlated. But still, it is possible to see pairs that seem logical to appear together in the corpus.</p>"
      ],
      "metadata": {
        "id": "fpC4Pi2EXhnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I will now perform the exact same analysis on the <code>test</code> and <code>val</code> datasets. We can expect the same behavior as the ones seen during the analysis of the training set, which is why I will refrain from commenting on the following plots to avoid redundancy. However, if something different appears, we will surely investigate further.</p>"
      ],
      "metadata": {
        "id": "rL1mtgivXhnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'test'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 32px; letter-spacing: 2.25px;color: #02011a;\"><b>Test Dataset</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "QQ3jXtJ4Xhnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting info on the test dataset\n",
        "describe_df(test)"
      ],
      "metadata": {
        "id": "MqKLGMO2Xhnq",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:35.106936Z",
          "iopub.execute_input": "2023-10-30T19:32:35.107258Z",
          "iopub.status.idle": "2023-10-30T19:32:35.131845Z",
          "shell.execute_reply.started": "2023-10-30T19:32:35.107232Z",
          "shell.execute_reply": "2023-10-30T19:32:35.13093Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'Id' from categorical features list\n",
        "categorical_features.remove('id')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "YYIFuRa6Xhnq",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:35.133222Z",
          "iopub.execute_input": "2023-10-30T19:32:35.133562Z",
          "iopub.status.idle": "2023-10-30T19:32:35.137872Z",
          "shell.execute_reply.started": "2023-10-30T19:32:35.133536Z",
          "shell.execute_reply": "2023-10-30T19:32:35.13668Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text_lenght = pd.DataFrame()\n",
        "for feat in categorical_features:\n",
        "    df_text_lenght[feat] = test[feat].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "histogram_boxplot(df_text_lenght,'#89c2e0', '#d500ff', 600, 1000, True, 'Test Dataset')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "Er-07gkmXhnq",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:35.138974Z",
          "iopub.execute_input": "2023-10-30T19:32:35.139282Z",
          "iopub.status.idle": "2023-10-30T19:32:35.304877Z",
          "shell.execute_reply.started": "2023-10-30T19:32:35.139255Z",
          "shell.execute_reply": "2023-10-30T19:32:35.303895Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english') # Top 15 terms\n",
        "x = vectorizer.fit_transform(test['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Unigrams', 'Test - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "-L9hBhHuXhnq",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:35.306254Z",
          "iopub.execute_input": "2023-10-30T19:32:35.306566Z",
          "iopub.status.idle": "2023-10-30T19:32:35.642178Z",
          "shell.execute_reply.started": "2023-10-30T19:32:35.306539Z",
          "shell.execute_reply": "2023-10-30T19:32:35.641179Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english') # Top 15 terms\n",
        "x = vectorizer.fit_transform(test['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Unigrams', 'Test - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "dVA5DOyOXhnq",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:35.643381Z",
          "iopub.execute_input": "2023-10-30T19:32:35.643688Z",
          "iopub.status.idle": "2023-10-30T19:32:35.916851Z",
          "shell.execute_reply.started": "2023-10-30T19:32:35.643662Z",
          "shell.execute_reply": "2023-10-30T19:32:35.915905Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (2,2)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(test['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Bigrams', 'Test - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "Wqnp9HI9Xhnq",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:47:47.92428Z",
          "iopub.execute_input": "2023-10-31T19:47:47.924837Z",
          "iopub.status.idle": "2023-10-31T19:47:48.458669Z",
          "shell.execute_reply.started": "2023-10-31T19:47:47.924789Z",
          "shell.execute_reply": "2023-10-31T19:47:48.457456Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (2,2)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(test['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Bigrams', 'Test - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "zPn-JMiuXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:47:55.64492Z",
          "iopub.execute_input": "2023-10-31T19:47:55.645358Z",
          "iopub.status.idle": "2023-10-31T19:47:55.94427Z",
          "shell.execute_reply.started": "2023-10-31T19:47:55.645327Z",
          "shell.execute_reply": "2023-10-31T19:47:55.94343Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (3,3)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(test['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Trigrams', 'Test - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "HmkS0No8Xhnr",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:48:00.143877Z",
          "iopub.execute_input": "2023-10-31T19:48:00.144277Z",
          "iopub.status.idle": "2023-10-31T19:48:00.628376Z",
          "shell.execute_reply.started": "2023-10-31T19:48:00.144248Z",
          "shell.execute_reply": "2023-10-31T19:48:00.627039Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (3,3)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(test['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Trigrams', 'Test - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "prrH3nRAXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:48:04.930546Z",
          "iopub.execute_input": "2023-10-31T19:48:04.930937Z",
          "iopub.status.idle": "2023-10-31T19:48:05.232923Z",
          "shell.execute_reply.started": "2023-10-31T19:48:04.930907Z",
          "shell.execute_reply": "2023-10-31T19:48:05.231768Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'val'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 32px; letter-spacing: 2.25px;color: #02011a;\"><b>Validation Dataset</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "uGPww2BvXhnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting info on the val dataset\n",
        "describe_df(val)"
      ],
      "metadata": {
        "id": "l3hIN_-GXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:37.24875Z",
          "iopub.execute_input": "2023-10-30T19:32:37.249382Z",
          "iopub.status.idle": "2023-10-30T19:32:37.275803Z",
          "shell.execute_reply.started": "2023-10-30T19:32:37.249341Z",
          "shell.execute_reply": "2023-10-30T19:32:37.27482Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'Id' from categorical features list\n",
        "categorical_features.remove('id')"
      ],
      "metadata": {
        "id": "MYeQg4NuXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:37.27705Z",
          "iopub.execute_input": "2023-10-30T19:32:37.27777Z",
          "iopub.status.idle": "2023-10-30T19:32:37.281671Z",
          "shell.execute_reply.started": "2023-10-30T19:32:37.277745Z",
          "shell.execute_reply": "2023-10-30T19:32:37.280618Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text_lenght = pd.DataFrame()\n",
        "for feat in categorical_features:\n",
        "    df_text_lenght[feat] = val[feat].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "histogram_boxplot(df_text_lenght,'#89c2e0', '#d500ff', 600, 1000, True, 'Validation Dataset')"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "i3-AsoOtXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:37.282792Z",
          "iopub.execute_input": "2023-10-30T19:32:37.283087Z",
          "iopub.status.idle": "2023-10-30T19:32:37.434514Z",
          "shell.execute_reply.started": "2023-10-30T19:32:37.283064Z",
          "shell.execute_reply": "2023-10-30T19:32:37.43355Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english') # Top 15 terms\n",
        "x = vectorizer.fit_transform(val['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Unigrams', 'Validation - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "MADrNF0AXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:37.445019Z",
          "iopub.execute_input": "2023-10-30T19:32:37.445358Z",
          "iopub.status.idle": "2023-10-30T19:32:37.777458Z",
          "shell.execute_reply.started": "2023-10-30T19:32:37.445332Z",
          "shell.execute_reply": "2023-10-30T19:32:37.7764Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english') # Top 15 terms\n",
        "x = vectorizer.fit_transform(val['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Unigrams', 'Validation - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "wXigAD_0Xhnr",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:37.77864Z",
          "iopub.execute_input": "2023-10-30T19:32:37.778963Z",
          "iopub.status.idle": "2023-10-30T19:32:38.021436Z",
          "shell.execute_reply.started": "2023-10-30T19:32:37.778936Z",
          "shell.execute_reply": "2023-10-30T19:32:38.020565Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (2,2)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(val['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Bigrams', 'Validation - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "FyDQ87iEXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:48:17.244652Z",
          "iopub.execute_input": "2023-10-31T19:48:17.245784Z",
          "iopub.status.idle": "2023-10-31T19:48:17.715931Z",
          "shell.execute_reply.started": "2023-10-31T19:48:17.245743Z",
          "shell.execute_reply": "2023-10-31T19:48:17.714789Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (2,2)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(val['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Bigrams', 'Validation - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "87zYMCW8Xhnr",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:48:21.511409Z",
          "iopub.execute_input": "2023-10-31T19:48:21.511906Z",
          "iopub.status.idle": "2023-10-31T19:48:21.820752Z",
          "shell.execute_reply.started": "2023-10-31T19:48:21.511868Z",
          "shell.execute_reply": "2023-10-31T19:48:21.819896Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (3,3)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(val['dialogue'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Trigrams', 'Validation - Dialogue', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "GFdHfm-LXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:48:30.484907Z",
          "iopub.execute_input": "2023-10-31T19:48:30.485303Z",
          "iopub.status.idle": "2023-10-31T19:48:30.964083Z",
          "shell.execute_reply.started": "2023-10-31T19:48:30.485273Z",
          "shell.execute_reply": "2023-10-31T19:48:30.962781Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 15,stop_words = 'english',ngram_range = (3,3)) # Top 15 terms\n",
        "x = vectorizer.fit_transform(val['summary'].fillna(''))\n",
        "df_tfidfvect = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "plot_correlation(df_tfidfvect, 'Trigrams', 'Validation - Summary', 800, 800, 12)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "57P03TjeXhnr",
        "execution": {
          "iopub.status.busy": "2023-10-31T19:48:34.65146Z",
          "iopub.execute_input": "2023-10-31T19:48:34.651893Z",
          "iopub.status.idle": "2023-10-31T19:48:34.956543Z",
          "shell.execute_reply.started": "2023-10-31T19:48:34.651857Z",
          "shell.execute_reply": "2023-10-31T19:48:34.955705Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Overall, we have similar patterns across all the three datasets. Summaries are shorter in length than dialogues‚Äîas expected‚Äîand lots of terms that seem reasonable to be together have a higher degree of correlation.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">By analyzing the <i>n-grams</i> heatmaps, it is also clear that this data consists of chat/dialogue texts, since we can see a lot of terms that would usuallly appear in conversations.</p>"
      ],
      "metadata": {
        "id": "hZSVBcpWXhnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'preprocess'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2.85px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 56px; letter-spacing: 2.25px;color: #02011a;\"><b>Preprocessing Data</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Pz-YiXDbXhnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">One of the main advantages of working with pre-trained models, such as BART, is that these models are usually extremely robust and require very little data preprocessing.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">While performing the EDA, I noticed that we have some tags in a few texts, such as <code>file_photo</code>. Let's take a look at a few examples.</p>"
      ],
      "metadata": {
        "id": "PR64vvisXhnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['dialogue'].iloc[14727])"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "h87UR7JSXhns",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.321164Z",
          "iopub.execute_input": "2023-10-30T19:32:39.321454Z",
          "iopub.status.idle": "2023-10-30T19:32:39.326929Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.321429Z",
          "shell.execute_reply": "2023-10-30T19:32:39.326037Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test['dialogue'].iloc[0])"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "9BwTCsYRXhns",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.328104Z",
          "iopub.execute_input": "2023-10-30T19:32:39.328815Z",
          "iopub.status.idle": "2023-10-30T19:32:39.339174Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.328778Z",
          "shell.execute_reply": "2023-10-30T19:32:39.338263Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I am going to use the <code>clean_tags</code> function defined below to remove these tags from the texts, so we can make them cleaner.</p>"
      ],
      "metadata": {
        "id": "HEjbHMJcXhns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tags(text):\n",
        "    clean = re.compile('<.*?>') # Compiling tags\n",
        "    clean = re.sub(clean, '', text) # Replacing tags text by an empty string\n",
        "\n",
        "    # Removing empty dialogues\n",
        "    clean = '\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n",
        "\n",
        "    return clean"
      ],
      "metadata": {
        "id": "r9L0WmlbXhns",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.340269Z",
          "iopub.execute_input": "2023-10-30T19:32:39.340537Z",
          "iopub.status.idle": "2023-10-30T19:32:39.355056Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.340515Z",
          "shell.execute_reply": "2023-10-30T19:32:39.354275Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = clean_tags(train['dialogue'].iloc[14727]) # Applying function to example text\n",
        "test2 = clean_tags(test['dialogue'].iloc[0]) # Applying function to example text\n",
        "\n",
        "# Printing results\n",
        "print(test1)\n",
        "print('\\n' *3)\n",
        "print(test2)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "7J33LJg6Xhns",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.356208Z",
          "iopub.execute_input": "2023-10-30T19:32:39.356544Z",
          "iopub.status.idle": "2023-10-30T19:32:39.367994Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.356518Z",
          "shell.execute_reply": "2023-10-30T19:32:39.367067Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">You can see that we have successfully removed the tags from the texts. I am now going to define the <code>clean_df</code> function, in which we will apply the <code>clean_tags</code> to the entire datasets.</p>"
      ],
      "metadata": {
        "id": "IJt_gzyFXhns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function to clean every text in the dataset.\n",
        "def clean_df(df, cols):\n",
        "    for col in cols:\n",
        "        df[col] = df[col].fillna('').apply(clean_tags)\n",
        "    return df"
      ],
      "metadata": {
        "id": "y5F6EbwKXhns",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.369072Z",
          "iopub.execute_input": "2023-10-30T19:32:39.369404Z",
          "iopub.status.idle": "2023-10-30T19:32:39.379725Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.369379Z",
          "shell.execute_reply": "2023-10-30T19:32:39.378929Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning texts in all datasets\n",
        "train = clean_df(train,['dialogue', 'summary'])\n",
        "test = clean_df(test,['dialogue', 'summary'])\n",
        "val = clean_df(val,['dialogue', 'summary'])"
      ],
      "metadata": {
        "id": "SNQnk2lTXhns",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.38124Z",
          "iopub.execute_input": "2023-10-30T19:32:39.381535Z",
          "iopub.status.idle": "2023-10-30T19:32:39.860513Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.381512Z",
          "shell.execute_reply": "2023-10-30T19:32:39.859405Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.tail(3) # Visualizing results"
      ],
      "metadata": {
        "id": "91Uo6h_CXhns",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.861733Z",
          "iopub.execute_input": "2023-10-30T19:32:39.862073Z",
          "iopub.status.idle": "2023-10-30T19:32:39.872505Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.862046Z",
          "shell.execute_reply": "2023-10-30T19:32:39.871412Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The tags have been removed from the texts. It's beneficial to conduct such data cleansing to eliminate noise‚Äîinformation that might not significantly contribute to the overall context and could potentially impair performance.</p>"
      ],
      "metadata": {
        "id": "D5SLlqPdXhns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I am now going to perform some preprocessing that is necessary to prepare our data to serve as input to the pre-trained model and for fine-tuning. Most of what I'm doing here is a part of the tutorial on Text Summarization described in the ü§ó Transformers documentation, which you can see <a href =\"https://huggingface.co/docs/transformers/tasks/summarization\">here</a>.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">First, I am going to use the ü§ó Datasets library to convert our Pandas Dataframes to Datasets. This is going to make our data ready to be processed across the whole Hugging Face ecosystem.</p>"
      ],
      "metadata": {
        "id": "uMg0zDOZXhns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming dataframes into datasets\n",
        "train_ds = Dataset.from_pandas(train)\n",
        "test_ds = Dataset.from_pandas(test)\n",
        "val_ds = Dataset.from_pandas(val)\n",
        "\n",
        "# Visualizing results\n",
        "print(train_ds)\n",
        "print('\\n' * 2)\n",
        "print(test_ds)\n",
        "print('\\n' * 2)\n",
        "print(val_ds)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "3D2WRBw_Xhnt",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.873973Z",
          "iopub.execute_input": "2023-10-30T19:32:39.874291Z",
          "iopub.status.idle": "2023-10-30T19:32:39.928278Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.874264Z",
          "shell.execute_reply": "2023-10-30T19:32:39.927358Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">To see the cotent inside a ü§ó Dataset, we can select a specific row, as below.</p>"
      ],
      "metadata": {
        "id": "aTp2gkx8Xhnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0] # Visualizing the first row"
      ],
      "metadata": {
        "id": "f44Z_JWiXhnu",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.929454Z",
          "iopub.execute_input": "2023-10-30T19:32:39.929744Z",
          "iopub.status.idle": "2023-10-30T19:32:39.937712Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.929713Z",
          "shell.execute_reply": "2023-10-30T19:32:39.936696Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">This way, we can see the original ID, the dialogue, as well as the reference summary. <code>__index_level_0__</code> does not add anything to the data and will be removed further.</p>"
      ],
      "metadata": {
        "id": "aQ8EzUZr3fgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">After successfully converting the pandas dataframes to ü§óDatasets, we can move on to the modeling process.</p>"
      ],
      "metadata": {
        "id": "8FCOoaVrXhnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'modeling'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2.85px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 56px; letter-spacing: 2.25px;color: #02011a;\"><b>Modeling</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "sKkxaykkXhnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">As I have previously mentioned, we are going to fine-tune a version of BART that has been trained on several news articles for text summarization, <a href =\"https://huggingface.co/facebook/bart-large-xsum\"><b>facebook/bart-large-xsum</b></a>.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I will briefly demonstrate this model by loading a summarization pipeline with it to show you how it works on news data.</p>"
      ],
      "metadata": {
        "id": "nfIjzF5bXhnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading summarization pipeline with the bart-large-cnn model\n",
        "summarizer = pipeline('summarization', model = 'facebook/bart-large-xsum')"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "id": "qVGVBcjjXhnu",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:39.938755Z",
          "iopub.execute_input": "2023-10-30T19:32:39.939065Z",
          "iopub.status.idle": "2023-10-30T19:32:54.068175Z",
          "shell.execute_reply.started": "2023-10-30T19:32:39.939039Z",
          "shell.execute_reply": "2023-10-30T19:32:54.067137Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">As an example, I am going to use the following news article, published on CNN on October 24<sup>th</sup>, 2023, <i><a href =\"https://edition.cnn.com/2023/10/24/europe/bobi-oldest-ever-dog-dies-intl-scli/index.html\">Bobi, the world‚Äôs oldest dog ever, dies aged 31</a></i>. Notice that this is a totally unseen news article that I'm passing to the model, so we can see how it performs.</p>"
      ],
      "metadata": {
        "id": "PPp_X39AXhnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = '''Bobi, the world‚Äôs oldest dog ever, has died after reaching the almost inconceivable age of 31 years and 165 days, said Guinness World Records (GWR) on Monday.\n",
        "His death at an animal hospital on Friday was initially announced by veterinarian Dr. Karen Becker.\n",
        "She wrote on Facebook that ‚Äúdespite outliving every dog in history, his 11,478 days on earth would never be enough, for those who loved him.‚Äù\n",
        "There were many secrets to Bobi‚Äôs extraordinary old age, his owner Leonel Costa told GWR in February. He always roamed freely, without a leash or chain, lived in a ‚Äúcalm, peaceful‚Äù environment and ate human food soaked in water to remove seasonings, Costa said.\n",
        "He spent his whole life in Conqueiros, a small Portuguese village about 150 kilometers (93 miles) north of the capital Lisbon, often wandering around with cats.\n",
        "Bobi was a purebred Rafeiro do Alentejo ‚Äì a breed of livestock guardian dog ‚Äì according to his owner. Rafeiro do Alentejos have a life expectancy of about 12-14 years, according to the American Kennel Club.\n",
        "But Bobi lived more than twice as long as that life expectancy, surpassing an almost century-old record to become the oldest living dog and the oldest dog ever ‚Äì a title which had previously been held by Australian cattle-dog Bluey, who was born in 1910 and lived to be 29 years and five months old.\n",
        "However, Bobi‚Äôs story almost had a different ending.\n",
        "When he and his three siblings were born in the family‚Äôs woodshed, Costa‚Äôs father decided they already had too many animals at home.\n",
        "Costa and his brothers thought their parents had taken all the puppies away to be destroyed. However, a few sad days later, they found Bobi alive, safely hidden in a pile of logs.\n",
        "The children hid the puppy from their parents and, by the time Bobi‚Äôs existence became known, he was too old to be put down and went on to live his record-breaking life.\n",
        "His 31st birthday party in May was attended by more than 100 people and a performing dance troupe, GWR said.\n",
        "His eyesight deteriorated and walking became harder as Bobi grew older but he still spent time in the backyard with the cats, rested more and napped by the fire.\n",
        "‚ÄúBobi is special because looking at him is like remembering the people who were part of our family and unfortunately are no longer here, like my father, my brother, or my grandparents who have already left this world,‚Äù Costa told GWR in May. ‚ÄúBobi represents those generations.‚Äù\n",
        "'''\n",
        "summarizer(news) # Using the pipeline to generate a summary of the text above"
      ],
      "metadata": {
        "id": "PHP4WiUBXhnu",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:54.069636Z",
          "iopub.execute_input": "2023-10-30T19:32:54.069961Z",
          "iopub.status.idle": "2023-10-30T19:32:59.472824Z",
          "shell.execute_reply.started": "2023-10-30T19:32:54.069935Z",
          "shell.execute_reply": "2023-10-30T19:32:59.47184Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">You can observe that the model is able to accurately produce a much shorter text consisting of the most relevant information present in the input text. This is a successful summarization.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">However, this model has been trained mainly on datasets consisting of several news articles from CNN and the Daily Mail, not on much dialogue data. This is why I'm going to fine-tune it with the SamSum dataset.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's go ahead and load BartTokenizer and BartForConditionalGeneration using the <i><b>facebook/bart-large-xsum</b></i> checkpoint.</p>"
      ],
      "metadata": {
        "id": "kBXpxEIyXhnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'facebook/bart-large-xsum' # Model\n",
        "tokenizer = BartTokenizer.from_pretrained(checkpoint) # Loading Tokenizer"
      ],
      "metadata": {
        "id": "TuK_J5JBXhnu",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:32:59.47411Z",
          "iopub.execute_input": "2023-10-30T19:32:59.474437Z",
          "iopub.status.idle": "2023-10-30T19:33:00.643553Z",
          "shell.execute_reply.started": "2023-10-30T19:32:59.474409Z",
          "shell.execute_reply": "2023-10-30T19:33:00.642417Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(checkpoint) # Loading Model"
      ],
      "metadata": {
        "id": "XcRF1eBDXhnu",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:00.645061Z",
          "iopub.execute_input": "2023-10-30T19:33:00.645467Z",
          "iopub.status.idle": "2023-10-30T19:33:07.026932Z",
          "shell.execute_reply.started": "2023-10-30T19:33:00.64543Z",
          "shell.execute_reply": "2023-10-30T19:33:07.026081Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We can also print below the architecture of the model.</p>"
      ],
      "metadata": {
        "id": "Mn9W_YUkXhnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model) # Visualizing model's architecture"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "m-YspbstXhnu",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:07.028275Z",
          "iopub.execute_input": "2023-10-30T19:33:07.028571Z",
          "iopub.status.idle": "2023-10-30T19:33:07.036285Z",
          "shell.execute_reply.started": "2023-10-30T19:33:07.028548Z",
          "shell.execute_reply": "2023-10-30T19:33:07.035331Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">It is possible to see that the models consist of an encoder and a decoder, we can see the Linear Layers, as well as the activation functions, which use $GeLU$, instead of the more typical $ReLU$.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">It is also interesting to observe the output layer, <b>lm_head</b>, which shows us that this model is ideal for generating outputs with a vocabulary size‚Äî<code>out_features=50264</code>‚Äîthis shows us that this architecture is adequate for summarization tasks, as well as other tasks, such as translation for example.</p>"
      ],
      "metadata": {
        "id": "4wCY3aOCXhnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Now we must preprocess our datasets and use BartTokenizer so that our data is legible for the BART model.</p>"
      ],
      "metadata": {
        "id": "WV0ksLNWXhnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The following <code>preprocess_function</code> can be directly copied from the ü§ó Transformers documentation, and it serves well to preprocess data for several NLP tasks. I am going to delve a bit deeper into how it preprocesses the data by explaining the steps it takes.</p>"
      ],
      "metadata": {
        "id": "IuuLPCVaXhnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style = \"margin-left: 25px;\">\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ <code>inputs = [doc for doc in examples[\"dialogue\"]]:</code></b> In this line, we are iterating over every <code>dialogue</code> in the dataset and saving them as input to the model.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ <code>model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        ":</code></b> Here, we are using the <code>tokenizer</code> to convert the input dialogues into tokens that can be easily understood by the BART model. The <code>truncation=True</code> parameter ensures that all dialogues have a maximum number of 1024 tokens, as defined by the <code>max_length</code> parameter.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ <code>labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True):</code></b> This line performs a very similar tokenization process as the one above. This time, however, it tokenizes the target variable, which is our summaries. Also, note that the max_length here is significantly lower, at 128. This implies that we expect summaries to be a much shorter text than that of dialogues.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ <code>model_inputs[\"labels\"] = labels[\"input_ids\"]:</code></b> This line is essentially adding the tokenized labels to the preprocessed dataset, alongside the tokenized inputs.</p>\n",
        "  \n",
        "</div>"
      ],
      "metadata": {
        "id": "y1Xf3SNHXhnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [doc for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "KJEOYsm2Xhnv",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:07.037767Z",
          "iopub.execute_input": "2023-10-30T19:33:07.038119Z",
          "iopub.status.idle": "2023-10-30T19:33:07.050079Z",
          "shell.execute_reply.started": "2023-10-30T19:33:07.038089Z",
          "shell.execute_reply": "2023-10-30T19:33:07.049174Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying preprocess_function to the datasets\n",
        "tokenized_train = train_ds.map(preprocess_function, batched=True,\n",
        "                               remove_columns=['id', 'dialogue', 'summary', '__index_level_0__']) # Removing features\n",
        "\n",
        "tokenized_test = test_ds.map(preprocess_function, batched=True,\n",
        "                               remove_columns=['id', 'dialogue', 'summary']) # Removing features\n",
        "\n",
        "tokenized_val = val_ds.map(preprocess_function, batched=True,\n",
        "                               remove_columns=['id', 'dialogue', 'summary']) # Removing features\n",
        "\n",
        "# Printing results\n",
        "print('\\n' * 3)\n",
        "print('Preprocessed Training Dataset:\\n')\n",
        "print(tokenized_train)\n",
        "print('\\n' * 2)\n",
        "print('Preprocessed Test Dataset:\\n')\n",
        "print(tokenized_test)\n",
        "print('\\n' * 2)\n",
        "print('Preprocessed Validation Dataset:\\n')\n",
        "print(tokenized_val)"
      ],
      "metadata": {
        "id": "m8gO1wOyXhnv",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:07.051143Z",
          "iopub.execute_input": "2023-10-30T19:33:07.051433Z",
          "iopub.status.idle": "2023-10-30T19:33:36.305648Z",
          "shell.execute_reply.started": "2023-10-30T19:33:07.051395Z",
          "shell.execute_reply": "2023-10-30T19:33:36.304711Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Our tokenized datasets consist now of only three features, <code>input_ids</code>, <code>attention_mask</code>, and <code>labels</code>. Let's print a sample from our tokenized train dataset to investigate further how the preprocess function altered the data.</p>"
      ],
      "metadata": {
        "id": "V7UJmz0-Xhnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting a sample from the dataset\n",
        "sample = tokenized_train[0]\n",
        "\n",
        "# Printing its features\n",
        "print(\"input_ids:\")\n",
        "print(sample['input_ids'])\n",
        "print(\"\\n\")\n",
        "print(\"attention_mask:\")\n",
        "print(sample['attention_mask'])\n",
        "print(\"\\n\")\n",
        "print(\"sample:\")\n",
        "print(sample['labels'])\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "-0-JXqeeXhnv",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:36.306879Z",
          "iopub.execute_input": "2023-10-30T19:33:36.307209Z",
          "iopub.status.idle": "2023-10-30T19:33:36.313994Z",
          "shell.execute_reply.started": "2023-10-30T19:33:36.307183Z",
          "shell.execute_reply": "2023-10-30T19:33:36.313096Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's dive a deep further into what each feature means.</p>"
      ],
      "metadata": {
        "id": "gxLwMxnaXhnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style = \"margin-left: 25px;\">\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ input_ids</b>: These are the token IDs mapped to the dialogues. Each token represents a word or subword that can be perfectly understood by the BART model. For instance, the number <i><b>5219</b></i> could be a map to a word like <i>\"hello\"</i> in BART's vocabulary. Each word has its unique token in this context.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ attention_mask</b>: This mask indicates which tokens the model should pay attention to and which tokens should be ignored. This is often used in the context of padding‚Äîwhen some tokens are used to equalize the lengths of sentences‚Äîbut most of these padding tokens do not hold any meaningful information, so the attention mask ensures the model does not focus on them. In the case of this specific sample, all tokens are masked as '1', meaning they are all relevant and none of them are used for padding.</p>\n",
        "    \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\"><b>‚Ä¢ labels</b>: Similarly to the first feature, these are token IDs obtained from the words and subwords in the summaries. These are the tokens that the model will be trained on to give as output.</p>\n",
        "  \n",
        "</div>"
      ],
      "metadata": {
        "id": "I-BN1h4GXhnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We must now use <code>DataCollatorForSeq2Seq</code> to batch the data. These data collators may also automatically apply some processing techniques, such as padding. They are important for the task of fine-tuning models and are also present in the ü§ó Transformers documentation for text summarization.</p>"
      ],
      "metadata": {
        "id": "0V4AzpwsXhnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "iGbF8TxCXhnv",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:36.315232Z",
          "iopub.execute_input": "2023-10-30T19:33:36.315512Z",
          "iopub.status.idle": "2023-10-30T19:33:36.329295Z",
          "shell.execute_reply.started": "2023-10-30T19:33:36.315466Z",
          "shell.execute_reply": "2023-10-30T19:33:36.328493Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Next, I am going to load the ROUGE metrics and define a new function to evaluate the model.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The <code>compute_metrics</code> function is also available in the documentation. In this function, we are basically extracting the model-generated summaries, as well as the human-generated summaries, and decoding them. We then use rouge to compare how similar they are to evaluate performance. </p>"
      ],
      "metadata": {
        "id": "K9gdPftUXhnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric('rouge') # Loading ROUGE Score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:36.330408Z",
          "iopub.execute_input": "2023-10-30T19:33:36.330678Z",
          "iopub.status.idle": "2023-10-30T19:33:36.732234Z",
          "shell.execute_reply.started": "2023-10-30T19:33:36.330655Z",
          "shell.execute_reply": "2023-10-30T19:33:36.731251Z"
        },
        "trusted": true,
        "id": "7GK0tFxg3fgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred# Obtaining predictions and true labels\n",
        "\n",
        "    # Decoding predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Obtaining the true labels tokens, while eliminating any possible masked token (i.e., label = -100)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "\n",
        "    # Computing rouge score\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()} # Extracting some results\n",
        "\n",
        "    # Add mean-generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "zwqspBG9Xhnv",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:36.733702Z",
          "iopub.execute_input": "2023-10-30T19:33:36.734436Z",
          "iopub.status.idle": "2023-10-30T19:33:36.744611Z",
          "shell.execute_reply.started": "2023-10-30T19:33:36.734397Z",
          "shell.execute_reply": "2023-10-30T19:33:36.743156Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We now use the <code>Seq2SeqTrainingArguments</code> class to set some relevant settings for fine-tuning. I will first define a directory to serve as output, and then define the evaluation strategy, learning rate, etc.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">This class can be quite extensive, with several different parameters. I highly suggest you take your time with <a href = \"https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\">the documentation</a> to get familiar with them.</p>"
      ],
      "metadata": {
        "id": "Bvpr4z26Xhnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining parameters for training\n",
        "'''\n",
        "Please don't forget to check the documentation.\n",
        "Both the Seq2SeqTrainingArguments and Seq2SeqTrainer classes have quite an extensive list of parameters.\n",
        "\n",
        "doc: https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/trainer\n",
        "\n",
        "'''\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir = 'bart_samsum',\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'eval_loss',\n",
        "    seed = seed,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "U6a7N1QIXhnv",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:36.746086Z",
          "iopub.execute_input": "2023-10-30T19:33:36.746507Z",
          "iopub.status.idle": "2023-10-30T19:33:36.765911Z",
          "shell.execute_reply.started": "2023-10-30T19:33:36.746469Z",
          "shell.execute_reply": "2023-10-30T19:33:36.765138Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Finally, the <code>Seq2SeqTrainer</code> class allows us to use <b>PyTorch</b> to fine-tune the model. In this class, we are basically defining the model, the training arguments, the datasets used for training and evaluation, the tokenizer, the data_collator, and the metrics.</p>"
      ],
      "metadata": {
        "id": "PtOfZP97Xhnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "PMmSY0JtXhnw",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:36.767175Z",
          "iopub.execute_input": "2023-10-30T19:33:36.768162Z",
          "iopub.status.idle": "2023-10-30T19:33:41.874277Z",
          "shell.execute_reply.started": "2023-10-30T19:33:36.768131Z",
          "shell.execute_reply": "2023-10-30T19:33:41.873478Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # Training model"
      ],
      "metadata": {
        "id": "bojj6fCcXhnw",
        "execution": {
          "iopub.status.busy": "2023-10-30T19:33:41.875419Z",
          "iopub.execute_input": "2023-10-30T19:33:41.875701Z",
          "iopub.status.idle": "2023-10-30T21:20:24.509952Z",
          "shell.execute_reply.started": "2023-10-30T19:33:41.875677Z",
          "shell.execute_reply": "2023-10-30T21:20:24.508961Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We finally finished fine-tuning after 4 epochs. Since we had <code>load_best_model_at_end = True</code> in the training arguments, the Trainer automatically saves the model with the best performance, which in this case is the one with the lowest <code>Validation Loss</code>.</p>\n",
        "\n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The second epoch was the one with the lowest validation loss, at <code><b>1.443861</b></code>. It also achieved the highest <code>Rouge1</code> and <code>Rouge2</code> scores, as well as the highest <code>Rougelsum</code> score.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I have not presented the <code><b>Rougelsum</b></code> score previously. According to <a href = \"https://pypi.org/project/rouge-score/\">the documentation</a> of the rouge-score library, we can conclude that this is similar to the RougeL score, but it measures content coverage at a sentence-by-sentence level, instead of the entire summary.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The <code>Gen Len</code> column gives us the average length of the model-generated summaries. It is relevant to remember that we want short, yet informative, texts. In this case, the second epoch also yielded the shortest summaries on average.</p>"
      ],
      "metadata": {
        "id": "sSpy6EnH3fgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'evaluating'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2.85px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 56px; letter-spacing: 2.25px;color: #02011a;\"><b>Evaluating and Saving Model</b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "xmGJHjAS3fgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">After training and testing the model, we can evaluate its performance on the <code>validation</code> dataset. We can use the <code>evaluate</code> method for that.</p>"
      ],
      "metadata": {
        "id": "hXqcYGFr3fgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating model performance on the tokenized validation dataset\n",
        "validation = trainer.evaluate(eval_dataset = tokenized_val)\n",
        "print(validation) # Printing results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:20:24.511224Z",
          "iopub.execute_input": "2023-10-30T21:20:24.511518Z",
          "iopub.status.idle": "2023-10-30T21:25:14.782038Z",
          "shell.execute_reply.started": "2023-10-30T21:20:24.511493Z",
          "shell.execute_reply": "2023-10-30T21:25:14.781096Z"
        },
        "trusted": true,
        "id": "6CGIvdZc3fgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">This outputs the same scores we have previously seen during training and testing. Here, we can notice that we have even <b>higher</b> performance in every metric compared to the performance in the testing set. When it comes to <code>Gen Len</code>, we also have more concise summaries in the validation set.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Considering that our results seem to be satisfactory at this point, we can go ahead and use the <code>save_model</code> method to save our fine-tuned model in the <code>bart_finetuned_samsum</code> directory. We can also use the <code>shutil</code> package to save the model in a <i>zip</i> file.</p>"
      ],
      "metadata": {
        "id": "jFnM45953fgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model to a custom directory\n",
        "directory = \"bart_finetuned_samsum\"\n",
        "trainer.save_model(directory)\n",
        "\n",
        "# Saving model tokenizer\n",
        "tokenizer.save_pretrained(directory)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:25:14.783529Z",
          "iopub.execute_input": "2023-10-30T21:25:14.784199Z",
          "iopub.status.idle": "2023-10-30T21:25:17.251328Z",
          "shell.execute_reply.started": "2023-10-30T21:25:14.784162Z",
          "shell.execute_reply": "2023-10-30T21:25:17.25027Z"
        },
        "trusted": true,
        "id": "BXahKlxU3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model in .zip format\n",
        "shutil.make_archive('bart_finetuned_samsum', 'zip', '/kaggle/working/bart_finetuned_samsum')\n",
        "shutil.move('bart_finetuned_samsum.zip', '/kaggle/working/bart_finetuned_samsum.zip')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:25:17.252425Z",
          "iopub.execute_input": "2023-10-30T21:25:17.252722Z",
          "iopub.status.idle": "2023-10-30T21:26:49.421219Z",
          "shell.execute_reply.started": "2023-10-30T21:25:17.252691Z",
          "shell.execute_reply": "2023-10-30T21:26:49.420244Z"
        },
        "trusted": true,
        "id": "VCoKvRU-3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">After saving your model, you can easily <a href = \"https://huggingface.co/docs/hub/models-uploading\">upload it to Hugging Face Models</a> and use it on new datasets and texts.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The fine-tuned model we trained here is now available for everyone on Hugging Face, and you can have access to it by clicking on <a href = \"https://huggingface.co/luisotorres/bart-finetuned-samsum\">luisotorres/bart-finetuned-samsum</a>.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's load the model, using the summarization pipeline, and generate some summaries for human evaluation, where we evaluate if the model-generated summaries are accurate or not.</p>"
      ],
      "metadata": {
        "id": "CeKWNCLa3fgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading summarization pipeline and model\n",
        "summarizer = pipeline('summarization', model = 'luisotorres/bart-finetuned-samsum')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:26:49.422379Z",
          "iopub.execute_input": "2023-10-30T21:26:49.422689Z",
          "iopub.status.idle": "2023-10-30T21:27:03.110881Z",
          "shell.execute_reply.started": "2023-10-30T21:26:49.422663Z",
          "shell.execute_reply": "2023-10-30T21:27:03.109801Z"
        },
        "trusted": true,
        "id": "3r0HfTl13fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">After loading the pipeline, we can now produce some summaries. I'll first start by using examples from the validation dataset, so we can compare our model-generated summaries to the reference summaries.</p>"
      ],
      "metadata": {
        "id": "DkIGdOlK3fgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining a random example from the validation dataset\n",
        "val_ds[35]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:03.112354Z",
          "iopub.execute_input": "2023-10-30T21:27:03.112657Z",
          "iopub.status.idle": "2023-10-30T21:27:03.120628Z",
          "shell.execute_reply.started": "2023-10-30T21:27:03.112632Z",
          "shell.execute_reply": "2023-10-30T21:27:03.119513Z"
        },
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "57UWmziy3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"John: doing anything special?\\r\\nAlex: watching 'Millionaires' on tvn\\r\\nSam: me too! He has a chance to win a million!\\r\\nJohn: ok, fingers crossed then! :)\"\n",
        "summary = \"Alex and Sam are watching Millionaires.\"\n",
        "generated_summary = summarizer(text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:03.121917Z",
          "iopub.execute_input": "2023-10-30T21:27:03.122265Z",
          "iopub.status.idle": "2023-10-30T21:27:07.60935Z",
          "shell.execute_reply.started": "2023-10-30T21:27:03.122235Z",
          "shell.execute_reply": "2023-10-30T21:27:07.608512Z"
        },
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "id": "CXooeiCT3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original Dialogue:\\n')\n",
        "print(text)\n",
        "print('\\n' * 2)\n",
        "print('Reference Summary:\\n')\n",
        "print(summary)\n",
        "print('\\n' * 2)\n",
        "print('Model-generated Summary:\\n')\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:07.610521Z",
          "iopub.execute_input": "2023-10-30T21:27:07.610816Z",
          "iopub.status.idle": "2023-10-30T21:27:07.617906Z",
          "shell.execute_reply.started": "2023-10-30T21:27:07.610791Z",
          "shell.execute_reply": "2023-10-30T21:27:07.61554Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "E0c-8Grj3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">The model-generated summary is just a bit longer than the reference summary, but it still captures quite well the content of the dialogue.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's see another example.</p>"
      ],
      "metadata": {
        "id": "VOpj0V2X3fgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds[22]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:07.619252Z",
          "iopub.execute_input": "2023-10-30T21:27:07.619537Z",
          "iopub.status.idle": "2023-10-30T21:27:07.641907Z",
          "shell.execute_reply.started": "2023-10-30T21:27:07.619514Z",
          "shell.execute_reply": "2023-10-30T21:27:07.640952Z"
        },
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "WD8DIRD23fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Madison: Hello Lawrence are you through with the article?\\r\\nLawrence: Not yet sir. \\r\\nLawrence: But i will be in a few.\\r\\nMadison: Okay. But make it quick.\\r\\nMadison: The piece is needed by today\\r\\nLawrence: Sure thing\\r\\nLawrence: I will get back to you once i am through.\"\n",
        "summary = \"Lawrence will finish writing the article soon.\"\n",
        "generated_summary = summarizer(text)\n",
        "\n",
        "print('Original Dialogue:\\n')\n",
        "print(text)\n",
        "print('\\n' * 2)\n",
        "print('Reference Summary:\\n')\n",
        "print(summary)\n",
        "print('\\n' * 2)\n",
        "print('Model-generated Summary:\\n')\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:07.642876Z",
          "iopub.execute_input": "2023-10-30T21:27:07.643191Z",
          "iopub.status.idle": "2023-10-30T21:27:12.262035Z",
          "shell.execute_reply.started": "2023-10-30T21:27:07.643147Z",
          "shell.execute_reply": "2023-10-30T21:27:12.261085Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "36msUKzX3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Once again, the model-generated summary is longer than the reference summary. However, I would definitely say that the model-generated summary is more informative than the reference one because it lets us know that there's a sense of urgency for Lawrence to finish the article since Madison needs it by today.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Let's see another example.</p>"
      ],
      "metadata": {
        "id": "PxRdqEDX3fgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds[4]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:12.263354Z",
          "iopub.execute_input": "2023-10-30T21:27:12.263944Z",
          "iopub.status.idle": "2023-10-30T21:27:12.270303Z",
          "shell.execute_reply.started": "2023-10-30T21:27:12.263907Z",
          "shell.execute_reply": "2023-10-30T21:27:12.269405Z"
        },
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "d54V83E43fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Robert: Hey give me the address of this music shop you mentioned before\\r\\nRobert: I have to buy guitar cable\\r\\nFred: Catch it on google maps\\r\\nRobert: thx m8\\r\\nFred: ur welcome\"\n",
        "summary = \"Robert wants Fred to send him the address of the music shop as he needs to buy guitar cable.\"\n",
        "generated_summary = summarizer(text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:12.271426Z",
          "iopub.execute_input": "2023-10-30T21:27:12.271767Z",
          "iopub.status.idle": "2023-10-30T21:27:14.702544Z",
          "shell.execute_reply.started": "2023-10-30T21:27:12.271735Z",
          "shell.execute_reply": "2023-10-30T21:27:14.701551Z"
        },
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "id": "vHJEZxBF3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original Dialogue:\\n')\n",
        "print(text)\n",
        "print('\\n' * 2)\n",
        "print('Reference Summary:\\n')\n",
        "print(summary)\n",
        "print('\\n' * 2)\n",
        "print('Model-generated Summary:\\n')\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:14.703829Z",
          "iopub.execute_input": "2023-10-30T21:27:14.70415Z",
          "iopub.status.idle": "2023-10-30T21:27:14.709868Z",
          "shell.execute_reply.started": "2023-10-30T21:27:14.704125Z",
          "shell.execute_reply": "2023-10-30T21:27:14.7088Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "8G4aDb9m3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">In this case, while the generated text captures the essence of the dialogue, it suffers from a lack of clarity due to ambiguity. Specifically, the pronoun <i>he</i> creates uncertainty about whether Fred or Robert intends to buy the guitar cable. In the original dialogue, it is clearly specified that it is Robert the one who has to buy the cable.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Now that we have been able to compare summaries, we can create some dialogues and input them into the model to check how it performs on them.</p>"
      ],
      "metadata": {
        "id": "eVEcBvMk3fgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new dialogues for evaluation\n",
        "text = \"John: Hey! I've been thinking about getting a PlayStation 5. Do you think it is worth it? \\r\\nDan: Idk man. R u sure ur going to have enough free time to play it? \\r\\nJohn: Yeah, that's why I'm not sure if I should buy one or not. I've been working so much lately idk if I'm gonna be able to play it as much as I'd like.\"\n",
        "generated_summary = summarizer(text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:14.71102Z",
          "iopub.execute_input": "2023-10-30T21:27:14.711278Z",
          "iopub.status.idle": "2023-10-30T21:27:19.264657Z",
          "shell.execute_reply.started": "2023-10-30T21:27:14.711256Z",
          "shell.execute_reply": "2023-10-30T21:27:19.263827Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "iVuAdw_O3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original Dialogue:\\n')\n",
        "print(text)\n",
        "print('\\n' * 2)\n",
        "print('Model-generated Summary:\\n')\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:19.265868Z",
          "iopub.execute_input": "2023-10-30T21:27:19.267018Z",
          "iopub.status.idle": "2023-10-30T21:27:19.272294Z",
          "shell.execute_reply.started": "2023-10-30T21:27:19.266988Z",
          "shell.execute_reply": "2023-10-30T21:27:19.271297Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "5PQO3HqA3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">For this dialogue, I have decided to include some abbreviations such as <i>idk</i>‚Äîfor <i>I don't know</i>‚Äîand <i>r u</i>‚Äîfor <i>are you</i>‚Äî to observe how the model would interpret them.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We can see that the model has been able to successfully capture the essence of the dialogue and identify the main subject, which is John's uncertainty to buy a PlayStation 5 given the fact that he has so little time to play it.</p>"
      ],
      "metadata": {
        "id": "tFVXPcGw3fgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Camilla: Who do you think is going to win the competition?\\r\\nMichelle: I believe Jonathan should win but I'm sure Mike is cheating!\\r\\nCamilla: Why do you say that? Can you prove Mike is really cheating?\\r\\nMichelle: I can't! But I just know!\\r\\nCamilla: You shouldn't accuse him of cheating if you don't have any evidence to support it.\"\n",
        "generated_summary = summarizer(text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:19.273434Z",
          "iopub.execute_input": "2023-10-30T21:27:19.273741Z",
          "iopub.status.idle": "2023-10-30T21:27:21.618283Z",
          "shell.execute_reply.started": "2023-10-30T21:27:19.273717Z",
          "shell.execute_reply": "2023-10-30T21:27:21.617265Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "CfibRSIi3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original Dialogue:\\n')\n",
        "print(text)\n",
        "print('\\n' * 2)\n",
        "print('Model-generated Summary:\\n')\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T21:27:21.619615Z",
          "iopub.execute_input": "2023-10-30T21:27:21.620539Z",
          "iopub.status.idle": "2023-10-30T21:27:21.62546Z",
          "shell.execute_reply.started": "2023-10-30T21:27:21.62051Z",
          "shell.execute_reply": "2023-10-30T21:27:21.624519Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "-3A_VSdG3fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Once more the model captures the main theme of the conversation, which is Michelle's belief that Jonathan should win the competition, but that Mike may be cheating. Some further improvements could be made, though, such as including the information that Michelle cannot really show any evidence to support her belief that Mike is cheating.</p>"
      ],
      "metadata": {
        "id": "m2JqSYuL3fgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id = 'conclusion'\n",
        "     style=\"font-family: Calibri, serif; text-align: left;\">\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 2.85px solid #041445;\n",
        "               width: 100%;\n",
        "               margin-top: 62px;\n",
        "               margin-bottom: auto;\n",
        "               margin-left: 0;\">\n",
        "    <div style=\"font-size: 56px; letter-spacing: 2.25px;color: #02011a;\"><b>Conclusion and Deployment </b></div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "BR4h60Hz3fgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">In this notebook, we have explored how we can use <b><i>Large Language Models</i></b> for several tasks involving Natural Language Processing, more specifically, Text Summarization tasks.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We delved into how Hugging Face's Transformers, Evaluate, and Datasets can be used to leverage frameworks such as PyTorch to fine-tune pre-trained models with a large number of parameters. This type of technique is usually referred to as <b>transfer learning</b>, which allows Data Scientists and Machine Learning Engineers to exploit the knowledge gained from previous tasks to improve generalization on a new task.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">We used a BART model that has been already trained to perform summarization on news articles and fine-tuned it to perform summarizations of dialogues with the <b>SamSum</b> dataset.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Thanks to Hugging Face's Models and Spaces, I have uploaded this model online, and it is free for anyone to use on their own summarization tasks or further fine-tune it on other tasks. I highly suggest you visit the <a href = \"https://huggingface.co/luisotorres/bart-finetuned-samsum\">luisotorres/bart-finetuned-samsum</a> for more information on how to use this model.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I have also built a <b>web app</b> where you can use the model for the summarization of dialogues and news articles. Below, you can see some images of the web app, which is also available for free on <a href = \"https://huggingface.co/spaces/luisotorres/bart-text-summarization\">Bart Text Summarization</a>.</p>"
      ],
      "metadata": {
        "id": "vS76EsEH3fgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <img src = \"https://i.imgur.com/ZMIsCHL.png\">\n",
        "<p style = \"font-size: 16px;\n",
        "            font-family: 'Calibri', serif;\n",
        "            text-align: center;\n",
        "            margin-top: 10px;\">Example of Summarization of News Article</p>\n",
        "</center>"
      ],
      "metadata": {
        "id": "5Lt_u1OL3fgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <img src = \"https://i.imgur.com/fc48B0l.png\">\n",
        "<p style = \"font-size: 16px;\n",
        "            font-family: 'Calibri', serif;\n",
        "            text-align: center;\n",
        "            margin-top: 10px;\">Example of Summarization of Dialogue</p>\n",
        "</center>"
      ],
      "metadata": {
        "id": "uOnF58gx3fgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">I hope that this notebook serves as a good introduction for those interested in the use of LLMs for Natural Language Processing tasks, as well as for those who already work with them and are in search of refining their knowledge on the subject.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">This notebook took quite a while to be made and I highly appreciate your feedback on this work. Feel free to leave your comments, suggestions, and upvotes if you liked the content presented here.</p>\n",
        "          \n",
        "<p style=\"font-family: Calibri, serif; text-align: left;\n",
        "          font-size: 24px; letter-spacing: .85px;color: #02011a\">Thank you very much!</p>"
      ],
      "metadata": {
        "id": "5cIEW0vw3fgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr style=\"border: 0;\n",
        "           height: 1px;\n",
        "           border-top: 0.85px;\n",
        "           solid #b2b2b2\">\n",
        "           \n",
        "<div style=\"text-align: left;\n",
        "            color: #8d8d8d;\n",
        "            padding-left: 15px;\n",
        "            font-size: 14.25px;\">\n",
        "    Luis Fernando Torres, 2023 <br><br>\n",
        "    Let's connect!üîó<br>\n",
        "    <a href=\"https://www.linkedin.com/in/luuisotorres/\">LinkedIn</a> ‚Ä¢ <a href=\"https://medium.com/@luuisotorres\">Medium</a> ‚Ä¢ <a href = \"https://huggingface.co/luisotorres\">Hugging Face</a><br><br>\n",
        "</div>"
      ],
      "metadata": {
        "id": "lDgy0EEZXhnw"
      }
    }
  ]
}